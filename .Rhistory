install.packages(ggplot2)
install.packages("ggplot2")
install.packages("dplyr")
install.packages("MASS")
install.packages("devtools")
library(devtools)
install_github(liusi2019/btloda)
install_github("liusi2019/btloda")
setwd("~/Documents/machine_learning/June-05-2018/big_popu")
filenames<-list.files(path= "./")
length(filenames)
output <- do.call("cbind", lapply(filenames, read.csv, header = TRUE))
dim(output)
output.df <- as.data.frame(matrix(as.numeric(unlist(output)), nrow = 1000, ncol = 180, byrow = TRUE))
output.df[1:10, 1:20]
l = 3
lookat <- output.df[, (36*2 + 1):(36*3)]
dim(lookat)
summary(looat[36*2 + 4])
summary(lookat[36*2 + 4])
summary(lookat[,36*2 + 4])
summary(lookat[,4])
sum(lookat[,4] > 0.5)
summary(lookat[,1] - lookat[,2])
summary(lookat[,1] - lookat[,7])
summary(lookat[, 13] - lookat[, 19])
summary(lookat[,13])
summary(lookat[,19])
summary(lookat[,25])
summary(lookat[,16])
raw_width = lookat[, 16]
summary(raw_width)
raw_width[which(raw_width! = 0]
raw_width[which(raw_width! = 0)]
raw_width[which(raw_width!= 0)]
min(raw_width[which(raw_width!= 0)])
exact_width = output.df[, 34]
min(exact_width[which(raw_width!=0)]
)
setwd("~/Documents/machine_learning/May-06-2018")
new_rec <- rep(0, 1000)
## keep record of pure estimate after having new point
new_pure <- rep(0, 1000)
com_rec <- rep(0, 20000)
## keep record of estimate at every step
new_est <- rep(0, 20000)
ele_u_vec <- rep(0, 1000)
width_vec = rep(0, 1000)
upper_vec = rep(0, 1000)
lower_vec = rep(0, 1000)
y_or_n = rep(0, 1000)
for (i in 1:20000){
new_sample_matrix <- rmultinom(1, size = 1, prob = new_q_mass)
new_sample_index <- rep(c(1:n), new_sample_matrix)
new_rec[new_sample_index] = new_rec[new_sample_index] +1
com_rec[i] = new_sample_index
new_use = rep(c(1:n), new_rec)
current_sample = (as.integer(order_index[new_use]<= round(alpha * n))/n)/(new_q_mass[new_use])
new_est[i] = mean(current_sample)
if (new_rec[new_sample_index] ==1){
new_count = sum(new_rec!=0)
new_pure[new_count] = new_est[i]
if(new_count >= 31){
resamples <- lapply(1:10000, function(x) sample(current_sample, replace = T))
r.mean <- sapply(resamples, mean)
element_l = quantile(r.mean, delta/(2*((new_count - 29)*(new_count - 30))) )
element_u = quantile(r.mean, 1-delta/(2*((new_count - 29)*(new_count - 30))) )
n_l = floor(as.numeric(element_l) * n)
n_u = ceiling(as.numeric(element_u) * n)
bound_l = binom.test(n_l, n, p = 0.5, alternative = "greater", conf.level = 0.95)$conf.int[1]
bound_u = binom.test(min(n_u,n), n, p = 0.5, alternative = "less", conf.level = 0.95)$conf.int[2]
ele_u_vec[new_count] = bound_u
width_vec[new_count] = bound_u - bound_l ## keep record of the width of CI at each step
y_or_n[new_count] = (bound_u >= alpha) & (bound_l <= alpha)
}
}
}
datab <- as.numeric(unlist(read.csv("datab.csv", header = TRUE)))
datan <- as.numeric(unlist(read.csv("datan.csv", header = TRUE)))
alpha = 0.20
target_length = 0.02
alpha0_vec = c(0.10, 0.15, 0.20, 0.25, 0.30)
delta = 0.05
n = 1000
result = rep(0, 36*length(alpha0_vec))
order_datan <- sort(datan)
## density estimate of f_b
b1 = sm.density(datab,  display = "none", eval.points = order_datan)
## density estiamte of f_n
b2 = sm.density(datan,  display = "none", eval.points = order_datan)
library(sm)
order_datan <- sort(datan)
## density estimate of f_b
b1 = sm.density(datab,  display = "none", eval.points = order_datan)
## density estiamte of f_n
b2 = sm.density(datan,  display = "none", eval.points = order_datan)
l = 3
alpha0 = alpha0_vec[l]
## resulting estiamte of f_a
b3 = (b2$estimate - (1-alpha0) * b1$estimate)/alpha0
b4 = pmax(b3, 0)
# there could possibly be bugs here, because not necessarily all value will be positive after the first part
# how to avoid the issue of having the second bunch of zeros?
if (min(b4)<=0){
min_0 = min(which(b4 == 0))
b4[1:(min_0 -1)] = 0
b4[which(b4 ==0)] = min(b4[which(b4!=0)])
}
order_index <- order(datan)
new_q_mass <- b4/sum(b4)
new_rec <- rep(0, 1000)
## keep record of pure estimate after having new point
new_pure <- rep(0, 1000)
com_rec <- rep(0, 20000)
## keep record of estimate at every step
new_est <- rep(0, 20000)
ele_u_vec <- rep(0, 1000)
width_vec = rep(0, 1000)
upper_vec = rep(0, 1000)
lower_vec = rep(0, 1000)
y_or_n = rep(0, 1000)
for (i in 1:20000){
new_sample_matrix <- rmultinom(1, size = 1, prob = new_q_mass)
new_sample_index <- rep(c(1:n), new_sample_matrix)
new_rec[new_sample_index] = new_rec[new_sample_index] +1
com_rec[i] = new_sample_index
new_use = rep(c(1:n), new_rec)
current_sample = (as.integer(order_index[new_use]<= round(alpha * n))/n)/(new_q_mass[new_use])
new_est[i] = mean(current_sample)
if (new_rec[new_sample_index] ==1){
new_count = sum(new_rec!=0)
new_pure[new_count] = new_est[i]
if(new_count >= 31){
resamples <- lapply(1:10000, function(x) sample(current_sample, replace = T))
r.mean <- sapply(resamples, mean)
element_l = quantile(r.mean, delta/(2*((new_count - 29)*(new_count - 30))) )
element_u = quantile(r.mean, 1-delta/(2*((new_count - 29)*(new_count - 30))) )
n_l = floor(as.numeric(element_l) * n)
n_u = ceiling(as.numeric(element_u) * n)
bound_l = binom.test(n_l, n, p = 0.5, alternative = "greater", conf.level = 0.95)$conf.int[1]
bound_u = binom.test(min(n_u,n), n, p = 0.5, alternative = "less", conf.level = 0.95)$conf.int[2]
ele_u_vec[new_count] = bound_u
width_vec[new_count] = bound_u - bound_l ## keep record of the width of CI at each step
y_or_n[new_count] = (bound_u >= alpha) & (bound_l <= alpha)
}
}
}
if (min(width_vec[31:new_count])<= target_length){
stop_point = 30 + min(which(width_vec[31:new_count] <= target_length))
}else{
stop_point = new_count
}
stop_point
new_count
install.packages(c("devtools", "roxygen2", "testthat", "knitr"))
library(devtools)
library(roxygen2)
library(testthat)
library(knitr)
is.infinite((-1) * (10^100))
(-1) * (10^100)
is.infinite((-1) * (10^10000))
library(devtools)
library(roxygen2)
devtools::documents()
devtools::document()
setwd("~/Documents/github/btloda")
devtools::document()
install.packages("~/Documents/github/btloda.tar.gz", repos = NULL, type = "source")
?btloda
install.packages("~/Documents/github/btloda.tar.gz", repos = NULL, type = "source")
library(btloda)
?btloda
install.packages("~/Documents/github/btloda.tar.gz", repos = NULL, type = "source")
?btloda
remove.packages("btloda")
install.packages("~/Documents/github/btloda.tar.gz", repos = NULL, type = "source")
library(btloda)
?btloda
setwd("~/Documents/github/btloda")
devtools::document()
install.packages("~/Documents/github/btloda.tar.gz", repos = NULL, type = "source")
library(btolda)
library(btloda)
?btloda
devtools::document()
install.packages("~/Documents/github/btloda.tar.gz", repos = NULL, type = "source")
library(btloda)
?btloda
a.df = as.data.frame(matrix(rnorm(200, 0, 1), 20, 10))
dim(a.df)
summary(a.df)
str(a.df)
as.matrix(a.df)
devtools::document()
install.packages("~/Documents/github/btloda.tar.gz", repos = NULL, type = "source")
?btloda
?get_neg_ll_all_hist
devtools::document()
install.packages("~/Documents/github/btloda.tar.gz", repos = NULL, type = "source")
?btloda
?get_neg_ll_all_hist
library(histogram)
library(MASS)
setwd("~/Documents/machine_learning/June-09-2018")
big_nominal = base::matrix(rnorm(20000*9, 0, 1), nrow = 20000, ncol = 9)
big_anomaly = matrix(ncol = 9, nrow = 20000)
vmat = matrix(0, ncol = 9, nrow = 9)
diag(vmat) = 1
for(i in (1:20000)){
center = rep(0, 9)
if(rbinom(1, 1, 0.4)==1){
center[sample(9, 3,replace = F)] = 3
}else{
center[sample(9, 4,replace = F)] = 3
}
big_anomaly[i,] = mvrnorm(1, center, vmat)
}
## write them into .csv file for running iforest in linux
write.csv(big_nominal, file = paste("big_nominal.csv", sep = ""), row.names = FALSE)
write.csv(big_anomaly, file = paste("big_anomaly.csv", sep = ""), row.names = FALSE)
n = 1000
vpro = 10
alpha = vpro/100  ## anomaly proportion in the mixture data set
q = 0.05 ## targetting on q quantile
dat =  matrix(ncol = 9, nrow = 2*n)
vmat = matrix(0, ncol = 9, nrow = 9)
diag(vmat) = 1
for(i in (1:n*alpha)){
center = rep(0, 9)
if(rbinom(1, 1, 0.4)==1){
center[sample(9, 3,replace = F)] = 3
}else{
center[sample(9, 4,replace = F)] = 3
}
dat[i,] = mvrnorm(1, center, vmat)
}
nnrow <- round(n * (2 - alpha))
rvec <- rnorm(9 * nnrow, 0, 1)
## the top alpha * n points are alien points
dat[(round(n*alpha)+1):(2*n),] <- base::matrix(rvec, nrow = round(n*(2-alpha)), ncol = 9)
data1 <- dat[(n+1):(2*n),] #nominal data set
data2 <- dat[1:n,] #mixture data set
bt_out = btloda(data1,sparsity=NA, maxk=1000,inf_replace = log(1e-09))
setwd("~/Documents/github/btloda")
devtools::document()
devtools::document()
install.packages("~/Documents/github/btloda.tar.gz", repos = NULL, type = "source")
library(btloda)
bt_out = btloda(data1,sparsity=NA, maxk=1000, inf_replace = log(1e-09))
bt_out$oob_nll
bt_out$pvh$w
str(bt_out$pvh$w)
data1
dim(data1)
length(bt_out$pvh$hists)
bt_out$pvh$hists[1]
get_neg_ll_all_hist(data2, w, hists, inf_replace = log(1e-09))
get_neg_ll_all_hist(data2, bt_out$pvh$w, bt_out$pvh$hists, inf_replace = log(1e-09))
summary(get_neg_ll_all_hist(data2, bt_out$pvh$w, bt_out$pvh$hists, inf_replace = log(1e-09)))\
summary(get_neg_ll_all_hist(data2, bt_out$pvh$w, bt_out$pvh$hists, inf_replace = log(1e-09)))
bt_score_nominal = get_neg_ll_all_hist(big_nominal, bt_out$pvh$w, bt_out$pvh$hists, inf_replace = log(1e-09))
summary(bt_score_nominal)
bt_score_anomaly = get_neg_ll_all_hist(big_anomaly, bt_out$pvh$w, bt_out$pvh$hists, inf_replace = log(1e-09))
summary(bt_score_anomaly)
hist(bt_score_nominal)
hist(bt_score_anomaly)
install_github("liusi2019/btloda")
library(btloda)
?btloda
a = matrix(rnorm(200, 0, 1), nrow = 20, ncol = 10)
bt_out = btloda(a,sparsity=NA, maxk=1000, inf_replace = log(1e-09))
bt_out$oob_nll
?get_neg_ll_all_hist
b = matrix(rnorm(500), nrow = 50, ncol = 10)
get_neg_ll_all_hist(b, bt_out$pvh$w, bt_out$pvh$hists, inf_replace = log(1e-09))
devtools::document()
install_github("liusi2019/btloda")
library(btloda)
a = matrix(rnorm(200, 0, 1), nrow = 20, ncol = 10)
bt_out = btloda(a,sparsity=NA, maxk=1000, inf_replace = log(1e-09))
b = matrix(rnorm(500), nrow = 50, ncol = 10)
get_neg_ll_all_hist(b, bt_out$pvh$w, bt_out$pvh$hists, inf_replace = log(1e-09))
summary(bt_out)
summary(bt_out$oob_nll)
bt_out = btloda(data1,sparsity=NA, maxk=1000, keep=NULL, exclude=NULL, debug=F, inf_replace = log(1e-09))
bt_out = btloda(data1,sparsity=NA, maxk=1000,inf_replace = log(1e-09))
summary(bt_out$oob_nll)
bt_datan = get_neg_ll_all_hist(data2, bt_out$pvh$w, bt_out$pvh$hists, inf_replace = log(1e-09))
summary(bt_datan)
bt_score_nominal = get_neg_ll_all_hist(big_nominal, bt_out$pvh$w, bt_out$pvh$hists, inf_replace = log(1e-09))
bt_score_anomaly = get_neg_ll_all_hist(big_anomaly, bt_out$pvh$w, bt_out$pvh$hists, inf_replace = log(1e-09))
summary(bt_score_nominal)
summary(bt_score_anomaly)
?get_neg_ll_all_hist
install_github("liusi2019/btloda")
library(btloda)
